# -*- coding: utf-8 -*-
"""Decision Tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GGESYh9ANzwpu4hZsHyeCG4e0FjoRWlI
"""

nltk.download()

import csv
import re
import nltk
import string
import math
import numpy as np
import sklearn
from nltk.corpus import stopwords
from pprint import pprint
from termcolor import colored
from sklearn.metrics import precision_recall_fscore_support

#tempaltes for printing 
printReverseGreen = lambda x : print(colored(x,'green',attrs=['reverse','bold']))
printReverseRed = lambda x : print(colored(x,'red',attrs=['reverse','bold']))

def printDataset(data,row):
    for i in range(row):
        printReverseRed('Row '+str(i))
        for attr in data[i]:
            print(attr)

def createDataset(fname):
    #reading dataset
    lol=list(csv.reader(open(fname), delimiter=':'))
    tarClass=list(set([e[0] for e in lol]))
    tarClass.sort()
    tarClass_map=dict(zip(tarClass,range(len(tarClass))))
    printReverseGreen("Mapping for target class")
    print(tarClass_map) 
    #Mapping target class to numeric value
    data=[[tarClass_map[row[0]]," ".join(row[1].split()[1:]),None,None,None,None] for row in lol]
    #removing punctuation and numeric value from question and generating vocab
    vocab=[]
    for row in data:
        questn=[word.lower() for word in re.sub('[^a-zA-Z]', ' ',row[1]).split() if len(word)>1]
        row[2]=questn
        row[3]=len(questn)
        vocab.extend(questn)
    #creating ngram 
    ngram=list(nltk.ngrams(vocab,1))
    #counting a ngram
    ngramFreq={}
    for e in ngram:
        if " ".join(e) in ngramFreq:ngramFreq[" ".join(e)]+=1
        else:ngramFreq[" ".join(e)]=1
    #sorting in decreasing order 
    ngramFreq=sorted(ngramFreq.items(),key=lambda x:x[1],reverse=True)
    #selecting top 500 from sorted list
    moFreq500=[e[0] for e in ngramFreq[:500]]
    #converting ngramFreq again back to dictionary from list of tuples
    ngramFreq=dict(ngramFreq)
    printReverseGreen("Datatset Attribute")
    print("-"*120)
    print("| Targte class | Raw Question | Question After preprocessing | length of question | Lexical | syntactical |")
    print("-"*120)
    printReverseGreen("first 5 rows from dataset after processing the question")
    printDataset(data,5)
    #extracting lexical and syntactic data
    for row in data:
        lexical=[]
        syntax=[]
        for word in row[2]:
            if word in moFreq500:
                lexical.append(word)
                syntax.append(nltk.pos_tag([word])[0][1])
        row[4]=lexical
        row[5]=syntax
    printReverseGreen("first 5 rows from dataset after extracting lexical and syntactic data")
    printDataset(data,5)
    return tarClass_map,ngramFreq,moFreq500,data

def probWordGivenTarClass(data,word,column_no,tarClass,vocab_length):
    tarClass_rows=[questn for questn in data if questn[0]==tarClass]
    count_word_given_tarClass=sum([row[column_no].count(word) for row in tarClass_rows])
    return (count_word_given_tarClass+1)/(len(tarClass_rows)+vocab_length)

def probWordsGivenTarClasses(tarClasses,column_no,data,words):
    map_probWordGivenTarClass={}
    for word in words:
        map={}
        for tarClass in tarClasses:
            map[tarClass]=probWordGivenTarClass(data,word,column_no,tarClass,len(words))
        map_probWordGivenTarClass[word]=map
    return map_probWordGivenTarClass

def trainCalculateColProbs(data,mpwgtc,mppgtc):
    for row in data:
        row[4]=np.product([mpwgtc[word][row[0]] for word in row[4]])
        row[5]=np.product([mppgtc[word][row[0]] for word in row[5]])
    printReverseGreen("first 5 rows from training dataset after probability calculations")
    printDataset(data,5)
    return data

def testCalculateColProbs(data,mpwgtc,mppgtc):
    for row in data:
        row[4]=np.product([max(mpwgtc[word].values()) if word in mpwgtc else 1 for word in row[4]])
        row[5]=np.product([max(mppgtc[word].values()) if word in mppgtc else 1 for word in row[5]])
    printReverseGreen("first 5 rows from testing dataset after probability calculations")
    printDataset(data,5)
    return data

def bestSplit(data,algorithm):
    transpose_data=np.array(data).T.tolist()
    impurity_attribute=[]
    #print(len(data))
    for i in range(1,len(transpose_data)):
        data=transpose_data[i]
        data_target=list(zip(data,transpose_data[0]))
        unique_data=list(set(data))
        unique_data.sort()
        #print("unique_data",unique_data)
        split_position=[(unique_data[i]+unique_data[i+1])/2 for i in range(len(unique_data)-1)]
        if len(unique_data)==1:
            split_position=[unique_data[0]]
        #print("split positions",split_position)
        impurity=[]
        minGiniSplit=1
        for position in split_position:
            le=[]
            le_target_frequency={}
            gt=[]
            gt_target_frequency={}
            for row in data_target:
                if row[0]<=position:le.append(row)
                else:gt.append(row)
            for target in tarClass_map.values():
                le_target_frequency[target]=0
                gt_target_frequency[target]=0
            for row in le:
                le_target_frequency[row[1]]+=1
            for row in gt:
                gt_target_frequency[row[1]]+=1
            le.append(None)
            gt.append(None)
            #gini
            if algorithm=="gini":
                algorithm_le=1-sum([(e[1]/len(le))**2 for e in le_target_frequency.items()])
                algorithm_gt=1-sum([(e[1]/len(gt))**2 for e in gt_target_frequency.items()])
                algorithm_split=(len(le)/(len(le)+len(gt)))*algorithm_le+(len(gt)/(len(le)+len(gt)))*algorithm_gt
            #entropy
            if algorithm=="entropy":
                algorithm_le=sum([-1*(e[1]/len(le))*math.log2((e[1]+1)/len(le)) for e in le_target_frequency.items()])
                algorithm_gt=sum([-1*(e[1]/len(gt))*math.log2((e[1]+1)/len(gt)) for e in gt_target_frequency.items()])
                algorithm_split=(len(le)/(len(le)+len(gt)))*algorithm_le+(len(gt)/(len(le)+len(gt)))*algorithm_gt
            #classification error
            if algorithm=="ce":
                algorithm_le=1-max([e[1]/len(le) for e in le_target_frequency.items()])
                algorithm_gt=1-max([e[1]/len(gt) for e in gt_target_frequency.items()])
                algorithm_split=(len(le)/(len(le)+len(gt)))*algorithm_le+(len(gt)/(len(le)+len(gt)))*algorithm_gt
            tarClass=None
            '''
            if int(algorithm_split)==0:
                for e in tarClass_map.values():
                    if le_target_frequency[e]+gt_target_frequency[e]==len(data_target):
                        tarClass=e
            '''
            target_frequency=[[e,le_target_frequency[e]+gt_target_frequency[e]] for e in tarClass_map.values()]
            tarClass=sorted(target_frequency,key=lambda x:x[1],reverse=True)[0][0]
            impurity.append([i,position,algorithm_split,tarClass])
        #print("impurity",impurity)
        #print("tarClass",tarClass)
        impurity_attribute.append(sorted(impurity,key=lambda x:x[2])[0])
        #print("impurity attribute",impurity_attribute)
        #if tarClass:break
    return sorted(impurity_attribute,key=lambda x:x[2])[0]

class node:
    def __init__(self,data):
        self.attr=None
        self.val=None
        self.impurity=None
        self.data=data
        self.left=None
        self.right=None
        self.tarClass=None

def bulidTree(train_data,algorithm):
    start=node(train_data)
    level=[start]
    cnt=0
    while level:
        cur=level[0]
        #if len(cur.data)>1:
        decision=bestSplit(cur.data,algorithm)
        cur.attr=decision[0]
        cur.val=decision[1]
        cur.impurity=decision[2]
        cur.tarClass=decision[3]
        #if len(cur.data)==1:
        #cur.tarClass=cur.data[0][0]
        #cur.impurity=0
        if cur.impurity!=0:
            left=[row for row in cur.data if row[cur.attr]<=cur.val]
            right=[row for row in cur.data if row[cur.attr]>cur.val]
            if left:
                cur.left=node(left)
                level.append(cur.left)
            if right:
                cur.right=node(right)
                level.append(cur.right)
        level.pop(0)
        cnt+=1
        if cnt==20000:
           break
    return start

def decisionTreeClassifier(start,attributes):
    cur=start
    tarClass=None
    while cur and cur.attr:
        #print("current attribute",cur.attr)
        if attributes[cur.attr-1]<=cur.val:
            tarClass=cur.tarClass
            cur=cur.left
        else:
            tarClass=cur.tarClass
            cur=cur.right
    return tarClass

#creation of train dataset
tarClass_map,train_ngramFreq,train_moFreq500,train_data=createDataset("dt_train.csv")

#uniques pos from train data
unique_pos=set([e for row in train_data for e in row[-1]])

#mapper for word given all target classes
map_probWordGivenTarClass=probWordsGivenTarClasses(sorted(tarClass_map.values()),4,train_data,train_moFreq500)

#mapper for pos given all target classes
map_prob_pos_given_tarClass=probWordsGivenTarClasses(sorted(tarClass_map.values()),5,train_data,unique_pos)

#calculate the probabilities for lexical and syntactical attribute
train_data_after_prob_cal=trainCalculateColProbs(train_data,map_probWordGivenTarClass,map_prob_pos_given_tarClass)

#selecting only target class,length,lexical,synatax attribute
train_data=[[row[0],row[3],row[4],row[5]] for row in train_data_after_prob_cal]

#creation of test dataset
tarClass_map,test_ngramFreq,test_moFreq500,test_data=createDataset("dt_test.csv")

#unique pos from test data
unique_pos=set([e for row in test_data for e in row[-1]])

#calculate the probabilities for lexical and syntactical attribute
test_data_after_prob_cal=testCalculateColProbs(test_data,map_probWordGivenTarClass,map_prob_pos_given_tarClass)
actual_target=np.array(test_data_after_prob_cal).T.tolist()[0]
test_predicted_algorithm=[]

for algorithm in ["gini","entropy","ce"]:
    printReverseRed(algorithm)
    #build tree
    printReverseGreen("Building Tree ...")
    start=bulidTree(train_data,algorithm)
#     prediction
    printReverseGreen("Predicting test data ...")
    test_predicted=[]
    for questn in test_data_after_prob_cal:
        test_predicted.append(decisionTreeClassifier(start,[questn[3],questn[4],questn[5]]))
    cnt=0
    for i in range(len(actual_target)):
        if actual_target[i]==test_predicted[i]:
            cnt+=1
    print("Accuracy",cnt/len(test_predicted)*100)
    #calculating precision,recall,fscore
    metric=precision_recall_fscore_support(actual_target,test_predicted,labels=range(len(tarClass_map)))
    printReverseGreen(" "*10+"| ".join([e[0]+" "+str(e[1]) for e in tarClass_map.items()]))
    metric_name=['precision','recall','f-score','support']
    for i  in range(len(metric)):
        print(metric_name[i],metric[i].tolist())
    test_predicted_algorithm.append(test_predicted)

#Observe how many samples are mis-classified using gini index based
#model but correctly classified by mis-classification error and
#cross-entropy based model.
false_gini_true_entropy=0
false_gini_true_ce=0

for i in range(len(test_predicted)):
    if test_predicted_algo[0][i]!=actual_target[i] and test_predicted_algo[1][i]==actual_target[i]:
        false_gini_true_entropy+=1
    if test_predicted_algo[0][i]!=actual_target[i] and test_predicted_algo[2][i]==actual_target[i]:
        false_gini_true_ce+=1

print("samples misclassfied by Gini but correctly classified by entropy",false_gini_true_entropy)
print("samples misclassfied by Gini but correctly classified by Classification Error",false_gini_true_ce)